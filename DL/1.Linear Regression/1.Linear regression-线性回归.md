# Linear regression-线性回归

## 数据集-line_fit_data

```txt
x,y
0.8521031222662713,6.1302578056656785
0.6309219899845755,5.5773049749614385
0.8532992575017331,6.133248143754333
0.6562364837215672,5.640591209303918
0.7006920149403018,5.751730037350754
0.9437734369476657,6.3594335923691645
0.1303019939151081,4.32575498478777
0.660397464272618,5.650993660681545
0.39225608329782513,4.980640208244563
0.7730323806614636,5.932580951653659
0.6993741635507859,5.748435408876965
0.3355028482629334,4.838757120657333
0.7777426955915523,5.944356738978881
0.13054733681427233,4.326368342035681
0.25373184335045806,4.634329608376145
0.795165002992007,5.987912507480018
0.7659096292358207,5.914774073089552
0.7878534434198352,5.969633608549588
0.06912260568090678,4.172806514202267
0.17578317697551926,4.439457942438798
0.741750552861692,5.85437638215423
0.8636300099166488,6.159075024791623
0.5649345937304435,5.412336484326109
0.3739876039158445,4.9349690097896115
0.9705731951258915,6.426432987814729
0.5666265755898384,5.416566438974596
0.1026777838158629,4.256694459539657
0.27071154484333604,4.67677886210834
0.9124636553553198,6.2811591383883
0.9546854943357289,6.386713735839322
0.7019188221706085,5.754797055426521
0.7539090658046659,5.884772664511665
0.5813714055193067,5.453428513798267
0.6904456579628969,5.726114144907243
0.14515972881723793,4.362899322043095
0.27402253274799127,4.685056331869978
0.5685798654317775,5.421449663579444
0.3970915659363051,4.992728914840763
0.7908424567693801,5.97710614192345
0.4980397018304271,5.2450992545760675
0.3512605442514215,4.878151360628554
0.30567978779101657,4.764199469477541
0.2824362329407971,4.706090582351993
0.6435227719676686,5.608806929919171
0.19340262581129564,4.4835065645282395
0.18863620073165777,4.471590501829144
0.6757495438889903,5.689373859722476
0.0630223087182159,4.15755577179554
0.2922923164688378,4.730730791172094
0.5623545338349585,5.405886334587397
0.7158068710884028,5.789517177721007
0.666827953892543,5.667069884731358
0.07317183750943101,4.1829295937735775
0.654772967563067,5.6369324189076675
0.3565308034064414,4.891327008516104
0.21386369034561714,4.534659225864043
0.1578918448991865,4.394729612247966
0.460753511409832,5.15188377852458
0.60530384063832,5.5132596015958
0.07608601726386022,4.19021504315965
0.2965368507231576,4.741342126807894
0.667765773665791,5.669414434164477
0.5175945391730441,5.29398634793261
0.6204252377868301,5.551063094467075
0.19734839334261933,4.493370983356549
0.5864891090429855,5.4662227726074635
0.4708800852471968,5.177200213117992
0.1880661572818032,4.470165393204508
0.45323688960493813,5.133092224012345
0.6801459399711272,5.7003648499278174
0.004447585174065294,4.011118962935163
0.76139349404626,5.90348373511565
0.04131049454537661,4.103276236363442
0.5553835941886336,5.388458985471583
0.8210936952000237,6.05273423800006
0.21014569055740284,4.525364226393507
0.5538555363679064,5.3846388409197665
0.01653495202683175,4.0413373800670795
0.6637355540488328,5.659338885122082
0.7742519034899824,5.935629758724956
0.27942945869902147,4.698573646747554
0.1760327496765426,4.440081874191357
0.068597009612833,4.171492524032082
0.31348076593017427,4.783701914825436
0.006505417415974324,4.016263543539936
0.3094622984023553,4.773655746005888
0.203523019180882,4.508807547952205
0.9837588139410272,6.459397034852568
0.8022348776567957,6.00558719414199
0.3823298646538582,4.9558246616346455
0.702243276739764,5.75560819184941
0.6888388115114241,5.72209702877856
0.07194196092665661,4.179854902316642
0.09240829284736762,4.231020732118419
0.4021698454916396,5.005424613729099
0.4464173680568575,5.116043420142144
0.17458030342281217,4.43645075855703
0.6018877085414425,5.504719271353606
0.2639287417502343,4.659821854375585
0.08693287643337255,4.217332191083432

```

## Code-代码

```python
import os
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE"
# Due to the conflict of numpy and torch with the file of "libiomp5md.dll"

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# Load the CSV data
data = pd.read_csv('line_fit_data.csv')

# Extract x and y values from the dataframe
X = data['x'].values.reshape(-1, 1)
y = data['y'].values

# Convert the data to PyTorch tensors
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).float()

# Define the linear regression model
class LinearRegressionModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearRegressionModel, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)
    
    def forward(self, x):
        return self.linear(x)

# Instantiate the model
model = LinearRegressionModel(input_dim=1, output_dim=1)

# Define the loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Lists to store loss values
losses = []

# Train the model
num_epochs = 2000
for epoch in range(num_epochs):
    # Forward pass
    y_pred = model(X_tensor)
    loss = criterion(y_pred, y_tensor.unsqueeze(1))
    
    # Backward pass and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Store the loss value
    losses.append(loss.item())
    
    # Print the loss every 100 epochs
    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# After training, get the weights and bias
w, b = model.linear.weight.data.item(), model.linear.bias.data.item()

# Plot the loss curve
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Curve')
plt.show()
plt.savefig('loss_curve.png')

```

------

Result-结果：

Epoch [100/2000], Loss: 0.2676
Epoch [200/2000], Loss: 0.0160
Epoch [300/2000], Loss: 0.0112
Epoch [400/2000], Loss: 0.0088
Epoch [500/2000], Loss: 0.0069
Epoch [600/2000], Loss: 0.0054
Epoch [700/2000], Loss: 0.0042
Epoch [800/2000], Loss: 0.0033
Epoch [900/2000], Loss: 0.0026
Epoch [1000/2000], Loss: 0.0020
Epoch [1100/2000], Loss: 0.0016
Epoch [1200/2000], Loss: 0.0013
Epoch [1300/2000], Loss: 0.0010
Epoch [1400/2000], Loss: 0.0008
Epoch [1500/2000], Loss: 0.0006
Epoch [1600/2000], Loss: 0.0005
Epoch [1700/2000], Loss: 0.0004
Epoch [1800/2000], Loss: 0.0003
Epoch [1900/2000], Loss: 0.0002
Epoch [2000/2000], Loss: 0.0002

This result shows the variation of the loss value at each epoch. The loss represents the difference between the model's predicted output and the true labels. In this result, as training progresses, the loss gradually decreases, indicating that the model's predictions are getting closer to the true labels.

Specifically, from the 100th epoch to the 2000th epoch, the loss decreases from 0.2676 to 0.0002. This indicates that the model is gradually learning the relationship between the data, and its performance is improving over the course of training. The final loss value is very close to zero, suggesting that the model has accurately fit the data and can be used for making predictions on new data.

这个结果显示了在每个epoch中损失值的变化。损失值（loss）代表了模型预测的输出与真实标签之间的差异。在这个结果中，随着训练的进行，损失值逐渐减小，说明模型的预测结果逐渐接近真实标签。

具体来说，从第100个epoch到第2000个epoch，损失值从0.2676逐渐减小到0.0002。这表明模型在训练过程中逐渐学习到了数据之间的关系，并且模型的性能逐渐提升。最终的损失值非常接近于0，这说明模型已经相当准确地拟合了数据，并且可以用于对新的数据进行预测。

## Detailed explanation-详解


当你的数据集中包含一个自变量和一个因变量时，线性回归是一种常见的建模方法，它用于探索两个变量之间的关系。在你提供的代码中，你使用了PyTorch来实现一个简单的线性回归模型。下面我将对整个代码进行解释：

1. **数据加载与预处理**：

   ```python
   # 加载CSV数据
   data = pd.read_csv('line_fit_data.csv')
   
   # 从数据帧中提取自变量和因变量
   X = data['x'].values.reshape(-1, 1)
   y = data['y'].values
   ```

   This part of the code loads data from a CSV file using pandas and extracts the independent variable (X) and dependent variable (y) from the dataframe. The X variable is reshaped into a column vector.

   这部分代码首先使用 pandas 库的 `read_csv` 函数加载了一个CSV文件，该文件包含自变量和因变量的数据。然后通过 `values` 属性将自变量和因变量转换为 NumPy 数组，并且将自变量变形为列向量。

   `X = data['x'].values.reshape(-1, 1)` 这行代码之所以要调用 `reshape(-1, 1)` 是因为在大多数机器学习任务中，我们希望自变量 X 是一个二维数组，其中每行代表一个样本，每列代表一个特征。即使在这里只有一个自变量 x，我们也要确保它是一个二维数组，以便与 PyTorch 的张量操作兼容。

   `reshape(-1, 1)` 的作用是将原始的一维数组重新排列成一个列向量，其中 `-1` 表示该维度由 numpy 自动推断，而 `1` 表示该维度有一个元素。这样做的结果是将原始的一维数组重新排列成了一个列向量，这种形式更符合机器学习中的数据结构要求。

   因此，这行代码确保了 X 是一个列向量，即使在数据集中只有一个特征。

   当我们有一个一维数组 `x`，比如:

   ```python
   x = np.array([1, 2, 3, 4, 5])
   ```

   在机器学习任务中，通常我们会将每个样本表示为一行，每个特征表示为一列。即使我们只有一个特征，也需要将它表示为列向量。这就是为什么我们要使用 `reshape(-1, 1)` 来确保 `x` 是一个列向量。

   ```python
   X = x.reshape(-1, 1)
   ```

   这样，我们得到的 `X` 就是一个二维数组，其中每一行代表一个样本，每一列代表一个特征，即使在这个例子中，我们只有一个特征。 `X` 现在是一个列向量：

   ```python
   array([[1],
          [2],
          [3],
          [4],
          [5]])
   ```

   这种表示方式符合了机器学习中的数据结构要求，可以直接传递给模型进行训练。

   

   For the independent variable `X`:

   - When we reshape `X`, we're ensuring that it conforms to the expected format for machine learning tasks, where each sample is represented as a row and each feature as a column. Even if we only have one feature, it's still best practice to represent it as a column vector.
   - Reshaping `X` using `reshape(-1, 1)` ensures that it becomes a column vector, even if there's only one feature.

   For the dependent variable `y`:

   - In most cases, we don't need to reshape `y` because it typically remains as a one-dimensional vector, representing the target values for each sample.
   - In PyTorch, when we convert `y` to a tensor, it maintains its original shape, and no additional reshaping is required.

   Therefore, in summary:

   - We reshape `X` to ensure it's in the expected format for machine learning tasks.
   - We typically don't need to reshape `y`, as it usually remains as a one-dimensional vector representing the target values.

2. **数据转换为PyTorch张量**：

   ```python
   # 将数据转换为PyTorch张量
   X_tensor = torch.from_numpy(X).float()
   y_tensor = torch.from_numpy(y).float()
   ```

   Here, the NumPy arrays are converted into PyTorch tensors since PyTorch models and optimizers work with tensors.

   这部分代码将 NumPy 数组转换为 PyTorch 张量，因为 PyTorch 中的模型和优化器需要使用张量数据类型。

3. **定义线性回归模型**：

   ```python
   class LinearRegressionModel(nn.Module):
       def __init__(self, input_dim, output_dim):
           super(LinearRegressionModel, self).__init__()
           self.linear = nn.Linear(input_dim, output_dim)
   
       def forward(self, x):
           return self.linear(x)
   ```

   This section defines a simple linear regression model. It is a class inheriting from `nn.Module`, containing a linear layer (`nn.Linear`) with input dimension `input_dim` and output dimension `output_dim`. In the `forward` method, input data `x` is passed through the linear layer to get the output.

   这部分代码定义了一个简单的线性回归模型。它是一个继承自 `nn.Module` 的类，其中包含一个线性层（`nn.Linear`），其输入维度为 `input_dim`，输出维度为 `output_dim`。在 `forward` 方法中，输入数据 `x` 经过线性层计算得到输出。

4. **实例化模型、定义损失函数和优化器**：

   ```python
   # 实例化模型
   model = LinearRegressionModel(input_dim=1, output_dim=1)
   
   # 定义损失函数和优化器
   criterion = nn.MSELoss()
   optimizer = optim.SGD(model.parameters(), lr=0.01)
   ```

   In this part, the linear regression model is instantiated, and the mean squared error loss function (`nn.MSELoss`) and stochastic gradient descent optimizer (`optim.SGD`) are defined.

   在这部分代码中，你实例化了上面定义的线性回归模型，并且定义了均方误差损失函数和随机梯度下降优化器。

5. **模型训练**：

   ```python
   # 列表用于存储损失值
   losses = []
   
   # 训练模型
   num_epochs = 2000
   for epoch in range(num_epochs):
       # 前向传播
       y_pred = model(X_tensor)
       loss = criterion(y_pred, y_tensor.unsqueeze(1))
       
       # 反向传播和优化
       optimizer.zero_grad()
       loss.backward()
       optimizer.step()
   
       # 存储损失值
       losses.append(loss.item())
       
       # 每100个epochs打印损失值
       if (epoch+1) % 100 == 0:
           print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
   ```

   Here, the model is trained for a certain number of epochs. In each epoch, a forward pass is performed to compute the predicted values, then the loss is calculated using the mean squared error criterion. After that, gradients are computed and the optimizer updates the model parameters. Loss values are stored in a list, and every 100 epochs, the loss is printed.

   在这部分代码中，你迭代训练模型。在每个epoch中，模型进行前向传播，计算损失，然后进行反向传播和参数优化。损失值被记录在列表 `losses` 中，并且每隔100个epochs打印一次损失值。

6. **可视化训练过程中的损失值曲线**：

   ```python
   plt.plot(losses)
   plt.xlabel('Epoch')
   plt.ylabel('Loss')
   plt.title('Training Loss Curve')
   plt.show()
   ```

   Finally, the training loss curve is plotted using matplotlib to visualize how the loss changes over epochs.

   最后，你使用 matplotlib 库绘制了训练过程中的损失值曲线。

整个代码实现了一个简单的线性回归模型，并使用 PyTorch 进行训练和损失值的可视化。